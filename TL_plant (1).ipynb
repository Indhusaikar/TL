{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cffe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os          \n",
    "import cv2                                 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import *         \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.model_selection as model_selection\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e4c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def load_data(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = os.listdir(dataset)\n",
    "    \n",
    "    for label_index, folder in enumerate(class_names):\n",
    "        folder_path = os.path.join(dataset, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):  # check for image files only\n",
    "                img_path = os.path.join(folder_path, file)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Unable to read image: {img_path}\")\n",
    "                    continue  # skip bad images\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, (150, 150))\n",
    "                images.append(image)\n",
    "                labels.append(label_index)\n",
    "    \n",
    "    return np.array(images), np.array(labels), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62821a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'D:/Indhu/DATASET/fruit/images/'\n",
    "images, labels, class_names = load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b20004",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images, dtype = 'float32')\n",
    "labels = np.array(labels, dtype = 'int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e912423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1451e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = model_selection.train_test_split(images, labels, train_size=0.80,test_size=0.20, random_state=101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c049a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.unique(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d75438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step - accuracy: 0.1301 - loss: 4.6311 - val_accuracy: 0.5517 - val_loss: 1.3956\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step - accuracy: 0.5759 - loss: 1.5239 - val_accuracy: 0.7931 - val_loss: 0.6591\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step - accuracy: 0.7173 - loss: 1.0724 - val_accuracy: 0.7931 - val_loss: 1.0087\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step - accuracy: 0.8042 - loss: 0.7628 - val_accuracy: 0.8103 - val_loss: 0.7815\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 800ms/step - accuracy: 0.8995 - loss: 0.4257 - val_accuracy: 0.8276 - val_loss: 0.6506\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768ms/step - accuracy: 0.8771 - loss: 0.3567 - val_accuracy: 0.7931 - val_loss: 0.9893\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802ms/step - accuracy: 0.8800 - loss: 0.3664 - val_accuracy: 0.7586 - val_loss: 0.9230\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860ms/step - accuracy: 0.9442 - loss: 0.1777 - val_accuracy: 0.8103 - val_loss: 0.8381\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step - accuracy: 0.9524 - loss: 0.1154 - val_accuracy: 0.7931 - val_loss: 0.9612\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817ms/step - accuracy: 0.9442 - loss: 0.1948 - val_accuracy: 0.8103 - val_loss: 0.9698\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.8060 - loss: 1.1720\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the InceptionV3 base model (excluding the top classifier layers)\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=train_images.shape[1:])\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model\n",
    "InceptionV3_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(40, activation='softmax')  # assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "InceptionV3_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_InceptionV3 = InceptionV3_model.fit(train_images, train_labels,\n",
    "                                        batch_size=128,\n",
    "                                        epochs=10,\n",
    "                                        validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "InceptionV3_evaluate = InceptionV3_model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bee658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - accuracy: 0.0314 - loss: 3.8135 - val_accuracy: 0.0517 - val_loss: 2.9357\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.0872 - loss: 3.0106 - val_accuracy: 0.0517 - val_loss: 2.5331\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.0956 - loss: 2.5953 - val_accuracy: 0.0517 - val_loss: 2.3660\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1702 - loss: 2.3829 - val_accuracy: 0.0862 - val_loss: 2.2748\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1319 - loss: 2.4406 - val_accuracy: 0.1207 - val_loss: 2.2200\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1177 - loss: 2.4074 - val_accuracy: 0.1724 - val_loss: 2.2045\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1734 - loss: 2.2366 - val_accuracy: 0.1379 - val_loss: 2.1889\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1131 - loss: 2.3953 - val_accuracy: 0.1897 - val_loss: 2.1723\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1786 - loss: 2.2877 - val_accuracy: 0.1897 - val_loss: 2.1670\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1011 - loss: 2.3799 - val_accuracy: 0.2069 - val_loss: 2.1680\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 0.1693 - loss: 2.1687\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load ResNet50 base model without top layers (we'll customize the classifier part)\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=train_images.shape[1:])\n",
    "base_model.trainable = False  # freeze base for feature extraction (optional)\n",
    "\n",
    "# Build the model\n",
    "ResNet50_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(40, activation='softmax')  # for 40 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "ResNet50_model.compile(optimizer='adam', \n",
    "                       loss='sparse_categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_ResNet50 = ResNet50_model.fit(train_images, train_labels, \n",
    "                                      batch_size=128, \n",
    "                                      epochs=10, \n",
    "                                      validation_split=0.2)\n",
    "\n",
    "# Evaluate on test data\n",
    "ResNet50_evaluate = ResNet50_model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8429ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.0884 - loss: 3.8495 - val_accuracy: 0.3966 - val_loss: 1.8602\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.4733 - loss: 1.7844 - val_accuracy: 0.4138 - val_loss: 1.6022\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.6496 - loss: 1.1323 - val_accuracy: 0.5690 - val_loss: 1.2016\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.7530 - loss: 0.8244 - val_accuracy: 0.6034 - val_loss: 1.0623\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.8341 - loss: 0.5114 - val_accuracy: 0.6379 - val_loss: 0.9591\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.8852 - loss: 0.3609 - val_accuracy: 0.6552 - val_loss: 0.9830\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5s/step - accuracy: 0.9021 - loss: 0.3015 - val_accuracy: 0.6552 - val_loss: 0.9945\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5s/step - accuracy: 0.9190 - loss: 0.2507 - val_accuracy: 0.6552 - val_loss: 0.9254\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5s/step - accuracy: 0.9384 - loss: 0.1900 - val_accuracy: 0.6552 - val_loss: 0.8736\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5s/step - accuracy: 0.9686 - loss: 0.1263 - val_accuracy: 0.6724 - val_loss: 0.8584\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 608ms/step - accuracy: 0.7470 - loss: 0.8185\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load VGG16 without the top (classifier), using pretrained ImageNet weights\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "base_model.trainable = False  # Freeze the convolutional base\n",
    "\n",
    "# Build the full model\n",
    "VGG16_model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(40, activation='softmax')  # Your 40-class classifier\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "VGG16_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_VGG16 = VGG16_model.fit(train_images, train_labels,\n",
    "                                batch_size=128,\n",
    "                                epochs=10,\n",
    "                                validation_split=0.2)\n",
    "VGG16_evaluate = VGG16_model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42ace35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 609ms/step - accuracy: 0.0204 - loss: 3.5616 - val_accuracy: 0.1552 - val_loss: 2.7344\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468ms/step - accuracy: 0.1310 - loss: 2.5943 - val_accuracy: 0.1379 - val_loss: 2.3249\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453ms/step - accuracy: 0.1095 - loss: 2.2340 - val_accuracy: 0.2241 - val_loss: 2.2365\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step - accuracy: 0.1800 - loss: 2.2065 - val_accuracy: 0.1897 - val_loss: 2.1587\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440ms/step - accuracy: 0.1773 - loss: 2.1517 - val_accuracy: 0.1897 - val_loss: 2.1460\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step - accuracy: 0.1825 - loss: 2.1286 - val_accuracy: 0.2414 - val_loss: 2.1587\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457ms/step - accuracy: 0.1630 - loss: 2.0804 - val_accuracy: 0.1207 - val_loss: 2.1439\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471ms/step - accuracy: 0.2169 - loss: 2.1042 - val_accuracy: 0.2414 - val_loss: 2.0977\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step - accuracy: 0.2010 - loss: 2.1030 - val_accuracy: 0.2414 - val_loss: 2.0801\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454ms/step - accuracy: 0.2356 - loss: 2.0476 - val_accuracy: 0.2759 - val_loss: 2.0878\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2313 - loss: 2.0870\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define AlexNet-like architecture\n",
    "def build_alexnet(input_shape=(150, 150, 3), num_classes=40):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "alexnet_model = build_alexnet()\n",
    "\n",
    "# Freeze feature extraction layers (up to the flatten layer)\n",
    "for layer in alexnet_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "alexnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_alexnet = alexnet_model.fit(train_images, train_labels, batch_size=128, epochs=10, validation_split=0.2)\n",
    "\n",
    "alexnet_evaluate = alexnet_model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fadad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389ms/step - accuracy: 0.0262 - loss: 3.5077 - val_accuracy: 0.0690 - val_loss: 2.8789\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.1203 - loss: 2.7440 - val_accuracy: 0.0690 - val_loss: 2.7602\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.1072 - loss: 2.6365 - val_accuracy: 0.0690 - val_loss: 2.6506\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.1281 - loss: 2.5397 - val_accuracy: 0.0690 - val_loss: 2.5596\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.1151 - loss: 2.4539 - val_accuracy: 0.0690 - val_loss: 2.3903\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.1177 - loss: 2.3924 - val_accuracy: 0.0690 - val_loss: 2.3443\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.1310 - loss: 2.3333 - val_accuracy: 0.1207 - val_loss: 2.3153\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.0924 - loss: 2.3103 - val_accuracy: 0.0862 - val_loss: 2.2990\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.1232 - loss: 2.2840 - val_accuracy: 0.0862 - val_loss: 2.2925\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.1206 - loss: 2.2603 - val_accuracy: 0.0862 - val_loss: 2.2934\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0955 - loss: 2.2558\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the original LeNet model\n",
    "LeNet = tf.keras.Sequential()\n",
    "LeNet.add(tf.keras.layers.Conv2D(6, 5, activation='tanh', input_shape=train_images.shape[1:], padding='valid'))\n",
    "LeNet.add(tf.keras.layers.AveragePooling2D(2))\n",
    "LeNet.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "LeNet.add(tf.keras.layers.Conv2D(16, 5, activation='tanh'))\n",
    "LeNet.add(tf.keras.layers.AveragePooling2D(2))\n",
    "LeNet.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "LeNet.add(tf.keras.layers.Conv2D(120, 5, activation='tanh'))\n",
    "LeNet.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Freeze convolutional layers (feature extractor)\n",
    "for layer in LeNet.layers[:-2]:  # freezing all but Dense layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add / keep classifier layers\n",
    "LeNet.add(tf.keras.layers.Dense(84, activation='tanh'))\n",
    "LeNet.add(tf.keras.layers.Dense(40, activation='softmax'))  # 10 classes\n",
    "\n",
    "# Compile the model\n",
    "LeNet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_LeNet = LeNet.fit(train_images, train_labels, batch_size=128, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "LeNet_evaluate = LeNet.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc2b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - accuracy: 0.0512 - loss: 4.0513 - val_accuracy: 0.0517 - val_loss: 2.6897\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1141 - loss: 2.8043 - val_accuracy: 0.0862 - val_loss: 2.4321\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.0863 - loss: 2.7338 - val_accuracy: 0.1379 - val_loss: 2.2888\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1307 - loss: 2.7003 - val_accuracy: 0.1207 - val_loss: 2.2331\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1008 - loss: 2.6365 - val_accuracy: 0.0517 - val_loss: 2.2642\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.0910 - loss: 2.7295 - val_accuracy: 0.0517 - val_loss: 2.2970\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1339 - loss: 2.5199 - val_accuracy: 0.0690 - val_loss: 2.3095\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.0895 - loss: 2.5776 - val_accuracy: 0.0862 - val_loss: 2.3147\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1200 - loss: 2.4627 - val_accuracy: 0.1379 - val_loss: 2.3186\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1708 - loss: 2.3868 - val_accuracy: 0.1379 - val_loss: 2.3115\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Assume train_images, train_labels, test_images, test_labels are already loaded\n",
    "# train_images shape: (num_samples, 150, 150, 3)\n",
    "\n",
    "# Step 1: Resize and normalize images\n",
    "train_images_resized = tf.image.resize(train_images, [224, 224])\n",
    "train_images_resized = tf.cast(train_images_resized, tf.float32) / 255.0\n",
    "\n",
    "test_images_resized = tf.image.resize(test_images, [224, 224])\n",
    "test_images_resized = tf.cast(test_images_resized, tf.float32) / 255.0\n",
    "\n",
    "# Step 2: Load MobileNetV2 base model (without top layers)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')  # or weights=None for random\n",
    "\n",
    "base_model.trainable = False  # Freeze feature extractor\n",
    "\n",
    "# Step 3: Build full model with classifier\n",
    "model_mobilenet = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(40, activation='softmax')  # 40 classes\n",
    "])\n",
    "\n",
    "# Step 4: Compile\n",
    "model_mobilenet.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train\n",
    "history_mobile = model_mobilenet.fit(train_images_resized, train_labels,\n",
    "                                     epochs=10,\n",
    "                                     batch_size=128,\n",
    "                                     validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b94cdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - accuracy: 0.0846 - loss: 2.2992\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "mobilenet_evaluate = model_mobilenet.evaluate(test_images_resized, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec0e51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.0346 - loss: 3.6444 - val_accuracy: 0.0862 - val_loss: 3.0074\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1037 - loss: 2.9995 - val_accuracy: 0.1379 - val_loss: 2.6651\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1287 - loss: 2.6961 - val_accuracy: 0.1379 - val_loss: 2.5136\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.0788 - loss: 2.6420 - val_accuracy: 0.1379 - val_loss: 2.4307\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1348 - loss: 2.4909 - val_accuracy: 0.0517 - val_loss: 2.3765\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1540 - loss: 2.5135 - val_accuracy: 0.0862 - val_loss: 2.3273\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1281 - loss: 2.6269 - val_accuracy: 0.0862 - val_loss: 2.2827\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.0982 - loss: 2.4680 - val_accuracy: 0.0517 - val_loss: 2.2611\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.1342 - loss: 2.4816 - val_accuracy: 0.1379 - val_loss: 2.2512\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.0901 - loss: 2.5962 - val_accuracy: 0.1379 - val_loss: 2.2541\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - accuracy: 0.0846 - loss: 2.2512\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Assume train_images, train_labels, test_images, test_labels are already loaded\n",
    "# and your original image shape is (150, 150, 3)\n",
    "\n",
    "# Step 1: Resize the images to 224x224\n",
    "train_images_resized = tf.image.resize(train_images, [224, 224]).numpy()\n",
    "test_images_resized = tf.image.resize(test_images, [224, 224]).numpy()\n",
    "\n",
    "# Step 2: Load EfficientNetB0 without top layer\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "# Step 3: Add custom classification head\n",
    "model_efficient = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(40, activation='softmax')  # for 40 classes\n",
    "])\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model_efficient.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the model\n",
    "history_eff = model_efficient.fit(train_images_resized, train_labels,\n",
    "                                  epochs=10,\n",
    "                                  batch_size=128,\n",
    "                                  validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "efficient_evaluate = model_efficient.evaluate(test_images_resized, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1580b561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alexnet_accuracy</th>\n",
       "      <th>alexnet_val_accuracy</th>\n",
       "      <th>alexnet_loss</th>\n",
       "      <th>alexnet_val_loss</th>\n",
       "      <th>LeNet_accuracy</th>\n",
       "      <th>LeNet_val_accuracy</th>\n",
       "      <th>LeNet_loss</th>\n",
       "      <th>LeNet_val_loss</th>\n",
       "      <th>InceptionV3_accuracy</th>\n",
       "      <th>InceptionV3_val_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>VGG16_loss</th>\n",
       "      <th>VGG16_val_loss</th>\n",
       "      <th>mobile_accuracy</th>\n",
       "      <th>mobile_val_accuracy</th>\n",
       "      <th>mobile_loss</th>\n",
       "      <th>mobile_val_loss</th>\n",
       "      <th>efficient_accuracy</th>\n",
       "      <th>efficient_val_accuracy</th>\n",
       "      <th>efficient_loss</th>\n",
       "      <th>efficient_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>3.489856</td>\n",
       "      <td>2.734374</td>\n",
       "      <td>0.039301</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>3.369551</td>\n",
       "      <td>2.878941</td>\n",
       "      <td>0.183406</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>...</td>\n",
       "      <td>3.643895</td>\n",
       "      <td>1.860204</td>\n",
       "      <td>0.061135</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>3.888184</td>\n",
       "      <td>2.689698</td>\n",
       "      <td>0.061135</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>3.888184</td>\n",
       "      <td>2.689698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122271</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.548240</td>\n",
       "      <td>2.324888</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.726383</td>\n",
       "      <td>2.760173</td>\n",
       "      <td>0.598253</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>...</td>\n",
       "      <td>1.781667</td>\n",
       "      <td>1.602200</td>\n",
       "      <td>0.104803</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.825704</td>\n",
       "      <td>2.432075</td>\n",
       "      <td>0.104803</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.825704</td>\n",
       "      <td>2.432075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113537</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>2.223398</td>\n",
       "      <td>2.236478</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.626623</td>\n",
       "      <td>2.650610</td>\n",
       "      <td>0.720524</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105974</td>\n",
       "      <td>1.201560</td>\n",
       "      <td>0.078603</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.760643</td>\n",
       "      <td>2.288837</td>\n",
       "      <td>0.078603</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.760643</td>\n",
       "      <td>2.288837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152838</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>2.221144</td>\n",
       "      <td>2.158702</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.536720</td>\n",
       "      <td>2.559558</td>\n",
       "      <td>0.807860</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815413</td>\n",
       "      <td>1.062278</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>2.727958</td>\n",
       "      <td>2.233126</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>2.727958</td>\n",
       "      <td>2.233126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187773</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>2.140616</td>\n",
       "      <td>2.146009</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.458143</td>\n",
       "      <td>2.390340</td>\n",
       "      <td>0.903930</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528558</td>\n",
       "      <td>0.959050</td>\n",
       "      <td>0.100437</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>2.634448</td>\n",
       "      <td>2.264170</td>\n",
       "      <td>0.100437</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>2.634448</td>\n",
       "      <td>2.264170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.187773</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>2.132528</td>\n",
       "      <td>2.158743</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.381214</td>\n",
       "      <td>2.344336</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362745</td>\n",
       "      <td>0.982958</td>\n",
       "      <td>0.109170</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>2.685128</td>\n",
       "      <td>2.297008</td>\n",
       "      <td>0.109170</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>2.685128</td>\n",
       "      <td>2.297008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.170306</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>2.076495</td>\n",
       "      <td>2.143857</td>\n",
       "      <td>0.122271</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>2.335067</td>\n",
       "      <td>2.315252</td>\n",
       "      <td>0.886463</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305550</td>\n",
       "      <td>0.994506</td>\n",
       "      <td>0.126638</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.528364</td>\n",
       "      <td>2.309526</td>\n",
       "      <td>0.126638</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2.528364</td>\n",
       "      <td>2.309526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.196507</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>2.125148</td>\n",
       "      <td>2.097675</td>\n",
       "      <td>0.091703</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.302622</td>\n",
       "      <td>2.299043</td>\n",
       "      <td>0.947598</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244713</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.087336</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.581459</td>\n",
       "      <td>2.314664</td>\n",
       "      <td>0.087336</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.581459</td>\n",
       "      <td>2.314664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.192140</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>2.114259</td>\n",
       "      <td>2.080090</td>\n",
       "      <td>0.122271</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.277509</td>\n",
       "      <td>2.292472</td>\n",
       "      <td>0.951965</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194929</td>\n",
       "      <td>0.873564</td>\n",
       "      <td>0.113537</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.462973</td>\n",
       "      <td>2.318599</td>\n",
       "      <td>0.113537</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.462973</td>\n",
       "      <td>2.318599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.240175</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>2.048615</td>\n",
       "      <td>2.087836</td>\n",
       "      <td>0.122271</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>2.259931</td>\n",
       "      <td>2.293377</td>\n",
       "      <td>0.947598</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134112</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>0.170306</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.382691</td>\n",
       "      <td>2.311496</td>\n",
       "      <td>0.170306</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>2.382691</td>\n",
       "      <td>2.311496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alexnet_accuracy  alexnet_val_accuracy  alexnet_loss  alexnet_val_loss  \\\n",
       "0          0.030568              0.155172      3.489856          2.734374   \n",
       "1          0.122271              0.137931      2.548240          2.324888   \n",
       "2          0.113537              0.224138      2.223398          2.236478   \n",
       "3          0.152838              0.189655      2.221144          2.158702   \n",
       "4          0.187773              0.189655      2.140616          2.146009   \n",
       "5          0.187773              0.241379      2.132528          2.158743   \n",
       "6          0.170306              0.120690      2.076495          2.143857   \n",
       "7          0.196507              0.241379      2.125148          2.097675   \n",
       "8          0.192140              0.241379      2.114259          2.080090   \n",
       "9          0.240175              0.275862      2.048615          2.087836   \n",
       "\n",
       "   LeNet_accuracy  LeNet_val_accuracy  LeNet_loss  LeNet_val_loss  \\\n",
       "0        0.039301            0.068966    3.369551        2.878941   \n",
       "1        0.117904            0.068966    2.726383        2.760173   \n",
       "2        0.117904            0.068966    2.626623        2.650610   \n",
       "3        0.117904            0.068966    2.536720        2.559558   \n",
       "4        0.117904            0.068966    2.458143        2.390340   \n",
       "5        0.117904            0.068966    2.381214        2.344336   \n",
       "6        0.122271            0.120690    2.335067        2.315252   \n",
       "7        0.091703            0.086207    2.302622        2.299043   \n",
       "8        0.122271            0.086207    2.277509        2.292472   \n",
       "9        0.122271            0.086207    2.259931        2.293377   \n",
       "\n",
       "   InceptionV3_accuracy  InceptionV3_val_accuracy  ...  VGG16_loss  \\\n",
       "0              0.183406                  0.551724  ...    3.643895   \n",
       "1              0.598253                  0.793103  ...    1.781667   \n",
       "2              0.720524                  0.793103  ...    1.105974   \n",
       "3              0.807860                  0.810345  ...    0.815413   \n",
       "4              0.903930                  0.827586  ...    0.528558   \n",
       "5              0.882096                  0.793103  ...    0.362745   \n",
       "6              0.886463                  0.758621  ...    0.305550   \n",
       "7              0.947598                  0.810345  ...    0.244713   \n",
       "8              0.951965                  0.793103  ...    0.194929   \n",
       "9              0.947598                  0.810345  ...    0.134112   \n",
       "\n",
       "   VGG16_val_loss  mobile_accuracy  mobile_val_accuracy  mobile_loss  \\\n",
       "0        1.860204         0.061135             0.051724     3.888184   \n",
       "1        1.602200         0.104803             0.086207     2.825704   \n",
       "2        1.201560         0.078603             0.137931     2.760643   \n",
       "3        1.062278         0.117904             0.120690     2.727958   \n",
       "4        0.959050         0.100437             0.051724     2.634448   \n",
       "5        0.982958         0.109170             0.051724     2.685128   \n",
       "6        0.994506         0.126638             0.068966     2.528364   \n",
       "7        0.925439         0.087336             0.086207     2.581459   \n",
       "8        0.873564         0.113537             0.137931     2.462973   \n",
       "9        0.858416         0.170306             0.137931     2.382691   \n",
       "\n",
       "   mobile_val_loss  efficient_accuracy  efficient_val_accuracy  \\\n",
       "0         2.689698            0.061135                0.051724   \n",
       "1         2.432075            0.104803                0.086207   \n",
       "2         2.288837            0.078603                0.137931   \n",
       "3         2.233126            0.117904                0.120690   \n",
       "4         2.264170            0.100437                0.051724   \n",
       "5         2.297008            0.109170                0.051724   \n",
       "6         2.309526            0.126638                0.068966   \n",
       "7         2.314664            0.087336                0.086207   \n",
       "8         2.318599            0.113537                0.137931   \n",
       "9         2.311496            0.170306                0.137931   \n",
       "\n",
       "   efficient_loss  efficient_val_loss  \n",
       "0        3.888184            2.689698  \n",
       "1        2.825704            2.432075  \n",
       "2        2.760643            2.288837  \n",
       "3        2.727958            2.233126  \n",
       "4        2.634448            2.264170  \n",
       "5        2.685128            2.297008  \n",
       "6        2.528364            2.309526  \n",
       "7        2.581459            2.314664  \n",
       "8        2.462973            2.318599  \n",
       "9        2.382691            2.311496  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = history_alexnet.history\n",
    "LeNet = history_LeNet.history\n",
    "InceptionV3 = history_InceptionV3.history\n",
    "ResNet50 = history_ResNet50.history\n",
    "VGG16 = history_VGG16.history\n",
    "mobile = history_mobile.history \n",
    "efficient = history_mobile.history \n",
    "\n",
    "\n",
    "comparison = pd.DataFrame()\n",
    "\n",
    "comparison['alexnet_accuracy']  =alexnet['accuracy']\n",
    "comparison['alexnet_val_accuracy'] = alexnet['val_accuracy']\n",
    "comparison['alexnet_loss']  = alexnet['loss']\n",
    "comparison['alexnet_val_loss'] = alexnet['val_loss']\n",
    "\n",
    "comparison['LeNet_accuracy']  = LeNet['accuracy']\n",
    "comparison['LeNet_val_accuracy'] = LeNet['val_accuracy']\n",
    "comparison['LeNet_loss']  = LeNet['loss']\n",
    "comparison['LeNet_val_loss'] = LeNet['val_loss']\n",
    "\n",
    "comparison['InceptionV3_accuracy']  = InceptionV3['accuracy']\n",
    "comparison['InceptionV3_val_accuracy'] =InceptionV3['val_accuracy']\n",
    "comparison['InceptionV3_loss']  = InceptionV3['loss']\n",
    "comparison['InceptionV3_val_loss'] = InceptionV3['val_loss']\n",
    "\n",
    "comparison['ResNet50_accuracy']  = ResNet50['accuracy']\n",
    "comparison['ResNet50_val_accuracy'] = ResNet50['val_accuracy']\n",
    "comparison['ResNet50_loss']  = ResNet50['loss']\n",
    "comparison['ResNet50_val_loss'] = ResNet50['val_loss']\n",
    "\n",
    "comparison['VGG16_accuracy']  = VGG16['accuracy']\n",
    "comparison['VGG16_val_accuracy'] = VGG16['val_accuracy']\n",
    "comparison['VGG16_loss']  = VGG16['loss']\n",
    "comparison['VGG16_val_loss'] = VGG16['val_loss']\n",
    "\n",
    "comparison['mobile_accuracy']  = mobile['accuracy']\n",
    "comparison['mobile_val_accuracy'] = mobile['val_accuracy']\n",
    "comparison['mobile_loss']  = mobile['loss']\n",
    "comparison['mobile_val_loss'] = mobile['val_loss']\n",
    "\n",
    "comparison['efficient_accuracy']  = efficient['accuracy']\n",
    "comparison['efficient_val_accuracy'] = efficient['val_accuracy']\n",
    "comparison['efficient_loss']  = efficient['loss']\n",
    "comparison['efficient_val_loss'] = efficient['val_loss']\n",
    "\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b6f2276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy =[alexnet_evaluate[1], LeNet_evaluate[1], InceptionV3_evaluate[1], ResNet50_evaluate[1], VGG16_evaluate[1], mobilenet_evaluate[1], efficient_evaluate[1]]\n",
    "lable = [\"AlexNet\" , \"LeNet\", \"InceptionV3\", \"ResNet50\", \"VGG16\", \"mobilenet\", \"efficient\"]\n",
    "\n",
    "barlist = plt.bar(lable,accuracy)\n",
    "\n",
    "barlist[0].set_color('orange')\n",
    "barlist[1].set_color('lime')\n",
    "barlist[2].set_color('blue')\n",
    "barlist[3].set_color('red')\n",
    "barlist[4].set_color('green')\n",
    "barlist[5].set_color('black')\n",
    "barlist[6].set_color('brown')\n",
    "\n",
    "\n",
    "plt.title(\" AlexNet Vs LeNet Vs InceptionV3 Vs ResNet50 Vs VGG16 Vs mobilenet Vs efficient\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece9bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
